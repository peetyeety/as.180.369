# Peer Comment on Bruce's Reflection

I really appreciated Bruce's honesty about where AI actually fails in his BNPL research. The example about Claude giving him generic regression specs that don't match his data constraints hit home—I've run into the same problem with my film tax credit research. When I asked Claude about control state selection for my difference-in-differences analysis, it suggested standard controls without understanding that I need states with similar pre-treatment motion picture industry trends, not just similar GDP. The model doesn't get the nuances of empirical identification strategy that separate decent research from rigorous research.

Bruce's distinction between AI as "research assistant" versus "collaborator" clarifies where I should draw the line in my own work. Claude has been genuinely helpful for organizing my literature review—it helped me see how Thom, Rickman & Wang, Bradbury, and Owens & Rennhoff connect to tell a coherent story about why film tax credits persist despite weak evidence. But when I need to think through whether my ACS migration data actually solves the reclassification problem I identified in QCEW data, or whether it just introduces different measurement issues, I'm completely on my own. AI can't evaluate whether my dual-data approach strengthens causal inference or just complicates it.

The most useful insight from both Bruce and Korinek is recognizing what AI absolutely cannot do: judge whether research questions matter or whether findings are significant. Bruce's example of the model claiming capital approached steady state while showing exponential growth is a perfect warning—AI can execute technical tasks correctly while fundamentally misunderstanding the substance. For my research, this means I should use AI heavily for data cleaning scripts and event study visualizations, but keep it far away from decisions about which economic controls to include or how to interpret null results. The 56% coding speedup Bruce mentions is real, but so is the risk of letting AI make analytical choices that require economic intuition.

What I'm taking from Bruce's reflection is the need to be more deliberate about documenting which parts of my research come from AI suggestions versus my own judgment. His note about pushing back on "AI-sounding" suggestions reminds me that the goal isn't just technically correct work, but authentic economic reasoning. I should probably start keeping a decision log of major methodological choices—not because anyone will read it, but because it forces me to own the research strategy instead of drifting toward whatever Claude recommends.
